"""
ê°•í™”ëœ íƒìƒ‰ì  ë°ì´í„° ë¶„ì„ (EDA) ìŠ¤í¬ë¦½íŠ¸
- ìƒì„¸í•œ íŒ¨í„´ ë¶„ì„
- ìƒê´€ê´€ê³„ ë¶„ì„
- ì‹œê³„ì—´ ë¶„ì„
- ì´ìƒì¹˜ íƒì§€
"""

import pandas as pd
import numpy as np
import json
from pathlib import Path
from typing import Dict, List, Any, Tuple
import warnings
from datetime import datetime, timedelta
import matplotlib
matplotlib.use('Agg')  # GUI ë°±ì—”ë“œ ì—†ì´ ì‚¬ìš©
import matplotlib.pyplot as plt
import seaborn as sns

warnings.filterwarnings('ignore')

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ
ROOT_DIR = Path(__file__).parent.parent.parent
DATA_DIR = ROOT_DIR / "data"
OUTPUT_DIR = ROOT_DIR / "src" / "output"
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

# í•œê¸€ í°íŠ¸ ì„¤ì • (matplotlib)
plt.rcParams['font.family'] = 'Malgun Gothic'
plt.rcParams['axes.unicode_minus'] = False


class EnhancedEDA:
    """ê°•í™”ëœ EDA í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.analysis_results = {}
        self.insights = []
    
    def load_data(self, file_path: Path, sheet_name: str) -> pd.DataFrame:
        """ì—‘ì…€ íŒŒì¼ì—ì„œ ë°ì´í„° ë¡œë“œ"""
        try:
            df = pd.read_excel(file_path, sheet_name=sheet_name)
            return df
        except Exception as e:
            print(f"ê²½ê³ : {sheet_name} ì½ê¸° ì‹¤íŒ¨: {e}")
            return pd.DataFrame()
    
    def analyze_correlation(self, df: pd.DataFrame, numeric_cols: List[str]) -> Dict:
        """ìƒê´€ê´€ê³„ ë¶„ì„"""
        if len(numeric_cols) < 2:
            return {}
        
        corr_matrix = df[numeric_cols].corr()
        
        # ê°•í•œ ìƒê´€ê´€ê³„ ì°¾ê¸° (ì ˆëŒ“ê°’ > 0.7)
        strong_corr = []
        for i in range(len(corr_matrix.columns)):
            for j in range(i+1, len(corr_matrix.columns)):
                val = corr_matrix.iloc[i, j]
                if abs(val) > 0.7 and not np.isnan(val):
                    strong_corr.append({
                        'col1': corr_matrix.columns[i],
                        'col2': corr_matrix.columns[j],
                        'correlation': float(val)
                    })
        
        return {
            'correlation_matrix': corr_matrix.to_dict(),
            'strong_correlations': strong_corr
        }
    
    def detect_outliers(self, df: pd.DataFrame, col: str) -> Dict:
        """ì´ìƒì¹˜ íƒì§€ (IQR ë°©ë²•)"""
        if col not in df.columns:
            return {}
        
        Q1 = df[col].quantile(0.25)
        Q3 = df[col].quantile(0.75)
        IQR = Q3 - Q1
        
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        
        outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]
        
        return {
            'outlier_count': len(outliers),
            'outlier_percentage': len(outliers) / len(df) * 100,
            'lower_bound': float(lower_bound),
            'upper_bound': float(upper_bound),
            'Q1': float(Q1),
            'Q3': float(Q3),
            'median': float(df[col].median())
        }
    
    def analyze_time_series_patterns(self, df: pd.DataFrame, 
                                    date_col: str, value_col: str) -> Dict:
        """ì‹œê³„ì—´ íŒ¨í„´ ë¶„ì„"""
        if date_col not in df.columns or value_col not in df.columns:
            return {}
        
        # ë‚ ì§œ íŒŒì‹±
        df[date_col] = pd.to_datetime(df[date_col], errors='coerce')
        df = df.dropna(subset=[date_col, value_col])
        
        if len(df) == 0:
            return {}
        
        # ì‹œê³„ì—´ ë¶„ì„
        patterns = {
            'trend': 'stable',  # ì¦ê°€/ê°ì†Œ/ì•ˆì •
            'seasonality': False,
            'cyclical': False,
            'mean': float(df[value_col].mean()),
            'std': float(df[value_col].std()),
            'min': float(df[value_col].min()),
            'max': float(df[value_col].max()),
            'range': float(df[value_col].max() - df[value_col].min())
        }
        
        # ì›”ë³„ íŒ¨í„´ í™•ì¸ (ê³„ì ˆì„±)
        df['month'] = df[date_col].dt.month
        monthly_means = df.groupby('month')[value_col].mean()
        if monthly_means.std() / monthly_means.mean() > 0.1:
            patterns['seasonality'] = True
        
        # íŠ¸ë Œë“œ ë¶„ì„ (ì„ í˜• ì¶”ì„¸)
        df_sorted = df.sort_values(date_col)
        if len(df_sorted) > 1:
            slope = np.polyfit(range(len(df_sorted)), df_sorted[value_col].values, 1)[0]
            if abs(slope) > patterns['mean'] * 0.01:
                patterns['trend'] = 'increasing' if slope > 0 else 'decreasing'
        
        return patterns
    
    def analyze_feature_importance_candidates(self, df: pd.DataFrame, 
                                              target_col: str) -> List[Dict]:
        """íŠ¹ì§• ì¤‘ìš”ë„ í›„ë³´ ë¶„ì„ (ìƒê´€ê´€ê³„ ê¸°ë°˜)"""
        if target_col not in df.columns:
            return []
        
        numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
        if target_col in numeric_cols:
            numeric_cols.remove(target_col)
        
        feature_scores = []
        for col in numeric_cols:
            try:
                corr = df[[col, target_col]].corr().iloc[0, 1]
                if not np.isnan(corr):
                    feature_scores.append({
                        'feature': col,
                        'correlation_with_target': float(abs(corr)),
                        'direction': 'positive' if corr > 0 else 'negative'
                    })
            except:
                continue
        
        # ìƒê´€ê´€ê³„ê°€ ë†’ì€ ìˆœìœ¼ë¡œ ì •ë ¬
        feature_scores.sort(key=lambda x: x['correlation_with_target'], reverse=True)
        
        return feature_scores[:20]  # ìƒìœ„ 20ê°œ
    
    def generate_insights(self, analysis_results: Dict) -> List[str]:
        """ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¸ì‚¬ì´íŠ¸ ìƒì„±"""
        insights = []
        
        # ë°ì´í„° í’ˆì§ˆ ì¸ì‚¬ì´íŠ¸
        total_rows = sum(v.get('ì „ì²´í†µê³„', {}).get('ì´í–‰ìˆ˜', 0) 
                        for v in analysis_results.values())
        if total_rows > 9000:
            insights.append(f"ğŸ“Š í’ë¶€í•œ ë°ì´í„°: ì´ {total_rows:,}í–‰ì˜ ë°ì´í„°ë¡œ ëª¨ë¸ í•™ìŠµ ê°€ëŠ¥")
        
        # ê²°ì¸¡ì¹˜ ì¸ì‚¬ì´íŠ¸
        missing_pct = 0
        for file_analysis in analysis_results.values():
            for sheet in file_analysis.get('ì‹œíŠ¸ëª©ë¡', []):
                missing_data = sheet.get('ê²°ì¸¡ë¥ ', {})
                if missing_data:
                    avg_missing = sum(v for v in missing_data.values() if isinstance(v, (int, float))) / len(missing_data)
                    missing_pct = max(missing_pct, avg_missing)
        
        if missing_pct < 5:
            insights.append("âœ… ë°ì´í„° í’ˆì§ˆ ìš°ìˆ˜: ê²°ì¸¡ì¹˜ê°€ 5% ë¯¸ë§Œìœ¼ë¡œ ëª¨ë¸ í•™ìŠµì— ì í•©")
        elif missing_pct < 10:
            insights.append("âš ï¸ ë°ì´í„° í’ˆì§ˆ ì–‘í˜¸: ê²°ì¸¡ì¹˜ê°€ 10% ë¯¸ë§Œìœ¼ë¡œ ì „ì²˜ë¦¬ í•„ìš”")
        else:
            insights.append("âŒ ë°ì´í„° í’ˆì§ˆ ê°œì„  í•„ìš”: ê²°ì¸¡ì¹˜ê°€ 10% ì´ìƒìœ¼ë¡œ ì „ì²˜ë¦¬ í•„ìˆ˜")
        
        # ì‹œê³„ì—´ íŒ¨í„´ ì¸ì‚¬ì´íŠ¸
        time_patterns = []
        for file_analysis in analysis_results.values():
            for sheet in file_analysis.get('ì‹œíŠ¸ëª©ë¡', []):
                if 'ì‹œê³„ì—´_íŒ¨í„´' in sheet:
                    patterns = sheet['ì‹œê³„ì—´_íŒ¨í„´']
                    if patterns.get('seasonality'):
                        time_patterns.append("ê³„ì ˆì„± íŒ¨í„´ ë°œê²¬")
                    if patterns.get('trend') != 'stable':
                        time_patterns.append(f"{patterns['trend']} íŠ¸ë Œë“œ ë°œê²¬")
        
        if time_patterns:
            insights.extend(time_patterns)
        
        return insights
    
    def comprehensive_analysis(self) -> Dict:
        """ì¢…í•© ë¶„ì„ ìˆ˜í–‰"""
        print("\n" + "="*60)
        print("ê°•í™”ëœ íƒìƒ‰ì  ë°ì´í„° ë¶„ì„ (Enhanced EDA) ì‹œì‘")
        print("="*60)
        
        # sheets.json ë¡œë“œ
        sheets_json_path = DATA_DIR / "sheets.json"
        if not sheets_json_path.exists():
            print("ê²½ê³ : sheets.json íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return {}
        
        with open(sheets_json_path, 'r', encoding='utf-8') as f:
            sheets_info = json.load(f)
        
        all_analysis = {}
        excel_files = [
            "1_ìƒí™œì¸êµ¬ë¶„ì„_LGìœ í”ŒëŸ¬ìŠ¤.xlsx",
            "1_ìƒí™œì¸êµ¬ë¶„ì„_ê²½ê¸°ë°ì´í„°ë“œë¦¼.xlsx",
            "3_ê´€ê´‘ì§€ë¶„ì„_LGìœ í”ŒëŸ¬ìŠ¤.xlsx",
            "3_ê´€ê´‘ì§€ë¶„ì„_ì‚¼ì„±ì¹´ë“œ.xlsx",
            "4_ê´€ê´‘ì§€íŠ¸ë Œë“œë¶„ì„_LGìœ í”ŒëŸ¬ìŠ¤.xlsx",
            "4_ê´€ê´‘ì§€íŠ¸ë Œë“œë¶„ì„_ì‚¼ì„±ì¹´ë“œ.xlsx",
            "5_ì‹í’ˆìœ„ìƒì—…ì†Œì†Œë¹„ë¶„ì„_ê²½ê¸°ë°ì´í„°ë“œë¦¼.xlsx",
            "5_ì‹í’ˆìœ„ìƒì—…ì†Œì†Œë¹„ë¶„ì„_ì‚¼ì„±ì¹´ë“œ.xlsx",
            "6_ì†Œë¹„ê²½ì œê·œëª¨ì¶”ì •.xlsx",
        ]
        
        for excel_file in excel_files:
            file_path = DATA_DIR / excel_file
            if not file_path.exists():
                continue
            
            print(f"\në¶„ì„ ì¤‘: {excel_file}")
            
            file_analysis = {
                "íŒŒì¼ëª…": excel_file,
                "ì‹œíŠ¸ëª©ë¡": []
            }
            
            # íŒŒì¼ì˜ ëª¨ë“  ì‹œíŠ¸ ì½ê¸°
            excel_file_obj = pd.ExcelFile(file_path)
            for sheet_name in excel_file_obj.sheet_names:
                df = self.load_data(file_path, sheet_name)
                
                if df.empty:
                    continue
                
                print(f"  - {sheet_name}: {len(df)}í–‰, {len(df.columns)}ì—´")
                
                sheet_analysis = {
                    "ì‹œíŠ¸ëª…": sheet_name,
                    "í–‰ìˆ˜": len(df),
                    "ì—´ìˆ˜": len(df.columns),
                    "ì—´ëª…": df.columns.tolist(),
                    "ê²°ì¸¡ë¥ ": (df.isnull().sum() / len(df) * 100).round(2).to_dict(),
                }
                
                # ìˆ«ìí˜• ì»¬ëŸ¼ ë¶„ì„
                numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
                if numeric_cols:
                    sheet_analysis["ìˆ«ìí˜•í†µê³„"] = df[numeric_cols].describe().to_dict()
                    
                    # ìƒê´€ê´€ê³„ ë¶„ì„
                    if len(numeric_cols) > 1:
                        corr_analysis = self.analyze_correlation(df, numeric_cols)
                        if corr_analysis:
                            sheet_analysis["ìƒê´€ê´€ê³„"] = corr_analysis
                    
                    # ì£¼ìš” ì»¬ëŸ¼ì— ëŒ€í•œ ì´ìƒì¹˜ íƒì§€
                    for col in numeric_cols[:3]:  # ì²˜ìŒ 3ê°œë§Œ
                        outlier_info = self.detect_outliers(df, col)
                        if outlier_info:
                            sheet_analysis.setdefault("ì´ìƒì¹˜", {})[col] = outlier_info
                    
                    # ì‹œê³„ì—´ ë¶„ì„ (ë‚ ì§œ ì»¬ëŸ¼ì´ ìˆëŠ” ê²½ìš°)
                    date_cols = [col for col in df.columns 
                               if 'ë‚ ì§œ' in str(col) or 'date' in str(col).lower() 
                               or 'ì›”' in str(col) or 'ì—°ë„' in str(col)]
                    if date_cols and len(numeric_cols) > 0:
                        date_col = date_cols[0]
                        value_col = numeric_cols[0]
                        time_series = self.analyze_time_series_patterns(df, date_col, value_col)
                        if time_series:
                            sheet_analysis["ì‹œê³„ì—´_íŒ¨í„´"] = time_series
                        
                        # íŠ¹ì§• ì¤‘ìš”ë„ í›„ë³´
                        feature_candidates = self.analyze_feature_importance_candidates(df, value_col)
                        if feature_candidates:
                            sheet_analysis["íŠ¹ì§•_ì¤‘ìš”ë„_í›„ë³´"] = feature_candidates[:10]
                
                file_analysis["ì‹œíŠ¸ëª©ë¡"].append(sheet_analysis)
            
            all_analysis[excel_file] = file_analysis
        
        # ì¢…í•© ì¸ì‚¬ì´íŠ¸ ìƒì„±
        insights = self.generate_insights(all_analysis)
        
        result = {
            "ë¶„ì„ì¼ì‹œ": datetime.now().isoformat(),
            "íŒŒì¼ë³„_ë¶„ì„": all_analysis,
            "ì¢…í•©_ì¸ì‚¬ì´íŠ¸": insights
        }
        
        # ê²°ê³¼ ì €ì¥
        output_file = OUTPUT_DIR / "enhanced_eda_results.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2, default=str)
        
        print(f"\n{'='*60}")
        print(f"ë¶„ì„ ì™„ë£Œ! ê²°ê³¼ ì €ì¥: {output_file}")
        print(f"{'='*60}")
        
        # ì¸ì‚¬ì´íŠ¸ ì¶œë ¥
        print("\nğŸ“Š ì£¼ìš” ì¸ì‚¬ì´íŠ¸:")
        for insight in insights:
            print(f"  {insight}")
        
        return result


if __name__ == "__main__":
    eda = EnhancedEDA()
    results = eda.comprehensive_analysis()
